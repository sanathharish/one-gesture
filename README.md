ğŸ¤ One-Gesture â€” Real-Time Hand Tracking & OS Interaction

A modular, extensible Python-based system for real-time hand- and finger-gesture control, featuring live visualization, gesture training, smoothing algorithms, and full OS automation support.
Designed for research, prototyping, accessibility, robotics, AR/VR, and desktop interaction.

ğŸš€ Features

ğŸ¥ Real-time webcam-based hand tracking

ğŸ· Gesture classification (static + dynamic patterns)

ğŸ–± OS control actions (mouse, keyboard, scroll, drag, hotkeys)

ğŸ§  Built-in gesture training studio (record, label, save, re-train)

ğŸ“Š Debug dashboard with FPS, gesture history, confidence, raw values

ğŸ”§ Modular design (replace MediaPipe with YOLO, TF Lite, OpenVINO, etc.)

ğŸ›‘ Safety controls: kill-gesture, calibration lock, manual override

ğŸ’» Streamlit prototype UI â†’ Flutter production UI pipeline

ğŸ“¦ Tech Stack
Component	Technology
Hand Detection	MediaPipe Hands (default), Ultralytics (optional)
Interface	Streamlit (debug), Flutter Desktop/Web (production)
Computer Vision	OpenCV
OS Automation	PyAutoGUI / pynput
ML Training	Scikit-Learn / TensorFlow Lite
Performance	Async loops, temporal smoothing (EMA/Kalman)

ğŸ“‚ Project Structure
one-gesture/
 â”œâ”€ main.py                    # Entry point
 â”œâ”€ requirements.txt
 â”œâ”€ README.md
 â”‚
 â”œâ”€ core/                     
 â”‚   â”œâ”€ detection.py           # Hand tracking + landmarks
 â”‚   â”œâ”€ calibration.py         # Screen mapping + scaling
 â”‚   â”œâ”€ smoothing.py           # Filtering + stabilization
 â”‚   â””â”€ gestures.py            # Rule-based gesture definitions
 â”‚
 â”œâ”€ os_actions/
 â”‚   â”œâ”€ mouse.py               # Cursor, click, drag
 â”‚   â””â”€ keyboard.py            # Typing, hotkeys
 â”‚
 â”œâ”€ training/
 â”‚   â”œâ”€ recorder.py            # Collect labeled gesture data
 â”‚   â”œâ”€ trainer.py             # Train ML classifier
 â”‚   â””â”€ models.pkl             # Saved user-defined gesture profiles
 â”‚
 â”œâ”€ ui/
 â”‚   â”œâ”€ streamlit_app.py       # Debug + experiment dashboard
 â”‚   â””â”€ flutter/               # Final production UI (later stage)
 â”‚
 â””â”€ utils/
     â””â”€ logger.py              # Action history + performance logs

ğŸ›  Installation
git clone https://github.com/sanathharish/one-gesture
cd one-gesture
pip install -r requirements.txt

â–¶ï¸ Run the Prototype
python main.py


or to run the Debug Dashboard:

streamlit run ui/streamlit_app.py

ğŸ§  Training New Gestures (Optional)
python training/recorder.py
python training/trainer.py

ğŸ§° Use Cases

Assistive technology (hands-free computing)

Human-computer interaction research

Robotics control

XR/Metaverse gesture interfaces

Gaming or music performance using gestures

ğŸ“„ License

MIT License â€” free to use, modify, and contribute.

ğŸ“š References

MediaPipe Hands Docs

OpenCV Python

Streamlit Docs

PyAutoGUI

Ultralytics YOLO Keypoint Models
